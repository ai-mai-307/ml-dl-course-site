## Описание задания

Дообучить LLM проще, чем кажется. В этом задании вам предстоит выбрать *маленькую* **большую** языковую модель (т.е число её параметров <= ~1.5 млрд.) и дообучить её на генерацию приколов (шуток, анекдотов, текстовых мемов) на русском языке путем параметро-эффективного тюнинга (рекомендуется выбрать LoRA).

## Постановка задачи

1. **Выбор модели**

    Выберите языковую модель подходящего объёма из HuggingFace Hub (порядка 1–2B параметров). Кратко зафиксируйте в ноутбуке:

    - название и размер модели;
    - почему вы считаете её подходящей для русскоязычного теста.

2. **Оценка исходных возможностей модели**

    Реализуйте базовый код генерации текста и проверьте, как модель в исходном, предобученном состоянии отвечает на запросы вида:
    > «Расскажи мне анекдот про …»

    Сформируйте небольшой набор таких промптов (например, 5–10 штук), выполните генерацию и сохраните ответы как **baseline** (выведите их в ноутбуке и при желании сохраните в таблицу или словарь).

3. **Выбор и подготовка текстового датасета**

    Выберите и подготовьте текстовый корпус для дообучения модели:

    - можно использовать предложенный датасет анекдотов или найти собственный корпус;
    - при необходимости выполните базовую очистку (уберите мусорные символы, артефакты переносов строк и т.п.);
    - приведите данные к формату, удобному для обучения LLM (например, пары «инструкция / промпт → текст анекдота»).

4. **Дообучение LLM с помощью LoRA**

    Настройте дообучение выбранной модели на подготовленном датасете, используя метод LoRA (через библиотеку `peft` или аналогичный инструмент):

    - оберните модель в LoRA-адаптер;
    - задайте разумные гиперпараметры (размер батча, число шагов/эпох, скорость обучения и т.п.);
    - запустите обучение и проследите, чтобы лосс на обучении и/или валидации вёл себя осмысленно (хотя бы несколько десятков/сотен шагов).

5. **Сохранение дообученной модели**

    Сохраните результат дообучения:

    - либо в виде LoRA-адаптера (рекомендуется),
    - либо в виде полной модели.  

    Убедитесь, что вы можете в отдельной ячейке/ноутбуке заново загрузить базовую модель + адаптер и выполнить инференс.

6. **Качественное сравнение «до» и «после»**

    Используя тот же набор промптов, что на шаге 2, сравните, как модель генерирует анекдоты **до** и **после** дообучения:

    - дайте короткий комментарий: стало ли лучше соответствие теме, структура анекдота, «человечность» текста и т.п.;
    - при желании дополнительно посчитайте простую количественную метрику (например, лосс/перплексию на валидационном наборе), но это не строго обязательно.

## Критерии оценки

### Обязательные условия

- аккуратный вид блокнота
- ссылка на Google таблицу с результатами экспериментов
- наличие текстовых ячеек с ходом рассуждений и интерпретаций экспериментов
- отсутствие подозрений в незадекларированном использовании LLM

### Начисления баллов

1. **Выбор модели и анализ её исходных возможностей (2 балла)**  
    - корректно выбрана и загружена LLM из HuggingFace Hub;  
    - показаны и кратко прокомментированы исходные ответы модели на промпты вида «Расскажи анекдот про …».

2. **Подготовка датасета (2 балла)**  
    - выбран и осмысленно подготовлен корпус анекдотов (или аналогичный текстовый датасет);  
    - данные приведены к удобному формату для обучения (есть понятные примеры в ноутбуке).

3. **Дообучение с помощью LoRA (3 балла)**  
    - модель обёрнута в LoRA-адаптер и корректно запускается процесс обучения;  
    - заданы внятные гиперпараметры;  
    - по логам видно, что обучение идёт осмысленно (лосс не взрывается, есть тенденция к снижению).

4. **Сравнение до/после и выводы (3 балла)**  
    - представлены примеры генерации анекдотов до и после дообучения по одним и тем же промптам;  
    - есть краткий текстовый анализ того, что изменилось в поведении модели.

---

# Рекомендации по выполнению практического задания №5

## Выбор модели и токенизатора

В рамках данной работы настоятельно рекомендуется воспользоваться ресурсами платформы [Hugging Face](https://huggingface.co/) и её библиотеки [transformers](https://huggingface.co/docs/transformers/index).

Эти программные инструменты предоставляют удобный поиск и использование существующих LLM.

В данной работе вам нужно выбрать модель, которая должна удовлетворять нескольким главным критериям:

- это decoder-only модель генерации текста;
- количество параметров должно находиться в пределах 1-2 млрд. параметров;
- в текстовом корпусе предобучения присутствует русский язык;
- это open-source модель, она скачивается и распространяется свободно.

Таким критериям соответствует[ TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0). При этом мы можете выбрать другую модель в [поиске](https://huggingface.co/models?pipeline_tag=text-generation&num_parameters=min:0,max:1B&language=en&sort=likes&search=llama).

В блокноте первыми в первых ячейках будет выполнить вход

```python
from huggingface_hub import notebook_login

notebook_login()

```

и загрузить локально веса модели:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "..."

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

```

## Первоначальный тест модели

Перед тем как приступать к дообучению, важно зафиксировать исходные возможности выбранной модели. Это позволит вам:

- получить baseline, относительно которого вы потом сможете показать улучшение качества генерации;
- проверить работоспособность пайплайна: токенизатор -> модель -> генерация -> декодирование. Это позволяет найти ошибки на раннем этапе и не тратить вычислительный бюджет на обучение “вслепую”.

### Проверка служебных токенов

Перед генерацией рекомендуется проверить, какие специальные токены используются в вашей модели:

- `bos_token` (начало последовательности);
- `eos_token` (конец последовательности);
- `pad_token` (паддинг при выравнивании батча).

В ряде decoder-only моделей `pad_token` отсутствует или совпадает с `eos_token`. Это не является ошибкой, однако может влиять на корректность некоторых процедур паддинга и логирования.

```python
print("bos:", tokenizer.bos_token, tokenizer.bos_token_id)
print("eos:", tokenizer.eos_token, tokenizer.eos_token_id)
print("pad:", tokenizer.pad_token, tokenizer.pad_token_id)
```

Если` pad_token` равен `None`, то для корректной работы паддинга (например, при батчевой генерации или обучении) допустимо назначить его равным `eos_token`:

```
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

### Мини-бенчмарк на 5–10 промптов

Для baseline-теста достаточно небольшого набора промптов (5–10 штук). Рекомендуется заранее выписать список тем (например, “про студентов”, “про программистов”, “про математику”, “про спорт”) и использовать их и до, и после дообучения.

Пример списка промптов:

```python
prompts = [
    "Расскажи анекдот про студентов!",
    "Расскажи анекдот про программистов!",
    "Расскажи анекдот про математику!",
    "Расскажи анекдот про футбол!",
    "Расскажи анекдот про Ленина!",
]

```

### Генерация текста: базовый шаблон

Допустим, вы выбрали chat модель. В таком случае она опрабатывает текстовые последотваельности со специальными служебными словами, обозначающие диалог, например: system, user, assistant. В таком случае следует  формировать запрос в виде такого диалога. Для этого удобно использовать метод `apply_chat_template`, который приведёт сообщения к корректному формату модели.

```python
import torch

system_msg = "Ты ассистент, который рассказывает короткие смешные анекдоты на русском языке."

def generate_one(prompt, temperature=0.7, top_p=0.9, max_new_tokens=120, repetition_penalty=1.1):
    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": prompt},
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,   # добавляет маркер начала ответа ассистента
    )

    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    return tokenizer.decode(out[0], skip_special_tokens=True)
```

Далее прогоните модель по списку промптов:

```python
for p in prompts:
    print("=" * 80)
    print("PROMPT:", p)
    print(generate_one(p, temperature=0.7, top_p=0.9, max_new_tokens=120))
```

### Варьирование параметров генерации

Качество генерации сильно зависит от параметров семплирования. Перед обучением имеет смысл попробовать 2–3 режима, чтобы понимать, как модель ведёт себя “из коробки”.

Рекомендуемые режимы:

- Более связный (меньше случайности): `temperature=0.6–0.8`, `top_p=0.6–0.9`
- Более разнообразный (больше случайности): `temperature=0.9–1.1`, `top_p=0.9–0.95`

Например:

```python
settings = [
    dict(temperature=0.7, top_p=0.6, max_new_tokens=120),
    dict(temperature=0.7, top_p=0.9, max_new_tokens=120),
    dict(temperature=1.0, top_p=0.95, max_new_tokens=150),
]

for cfg in settings:
    print("\n", "#" * 80)
    print("SETTINGS:", cfg)
    for p in prompts[:3]:
        print("-" * 80)
        print("PROMPT:", p)
        print(generate_one(p, **cfg))
```

### Что зафиксировать в тексте блокнота

После baseline-теста рекомендуется зафиксировать:

- выбранную модель (`model_id`);
- список промптов;
- параметры генерации (`temperature`, `top_p`, `max_new_tokens`, `repetition_penalty`);
- 5–10 примеров ответов модели (в основной таблице можно указать один самый лучший на ваш взгляд).

Эта часть нужна, чтобы в конце задания можно было корректно сравнить результаты до и после дообучения.

## Поиск и подготовка датасета

Качество результата дообучения в задаче генерации анекдотов в значительной степени определяется качеством данных. Практика показывает, что **лучше обучаться на меньшем объёме, но более качественных примерах**, чем на большом исходном неотфильтрованном корпусе.

В рамках данного задания вам необходимо подготовить датасет, содержащий пары вида:

- **prompt**: запрос пользователя (тема анекдота),
- **text**: ответ ассистента (сам анекдот).

При этом датасет должен удовлетворять следующим требованиям:

- тексты должны быть преимущественно на русском языке;
- тексты не должны быть слишком короткими или явно мусорными;
- желательно наличие признака качества (например, `rating`, количество лайков, “популярность”), чтобы вы могли фильтровать данные;
- формат данных должен позволять преобразовать запись в "диалог" `system → user → assistant`.

### Где искать датасеты

Рекомендуется рассмотреть следующие источники:

1. **Hugging Face Datasets** — каталог готовых датасетов, которые удобно загружать в ноутбук и преобразовывать в нужный формат.
2. **Kaggle / открытые репозитории** — часто содержат тематические наборы данных (в том числе юмористические), но могут потребовать более тщательной очистки.
3. **Собственный сбор (парсинг)** — возможен, но требует больше времени и усилий.

Например, вы можете найти на Kaggle датасет [Russian Jokes](https://www.kaggle.com/datasets/konstantinalbul/russian-jokes). В нем текстовые данные представлены в виде CSV таблицы, которую можно удобно предобработать с помощью библиотеки Pandas, добавляя нужные колонки.

> **Примечание:**это не идеальный и довольно грязный датасет!

### Критерии качества данных и их фильтрация

Перед форматированием рекомендуется выполнить **минимальный предварительный анализ датасета**:

1. посмотреть распределение качества (если есть `rating`);
2. посмотреть на распределение длины текстовых последовательностей;
3. выявить наличие мусорных символов и повторов.

Например, если у вас есть столбец `rating`, обычно имеет смысл:

- исключить примеры с наихудшими оценками;
- начать с поднабора, который заведомо лучше (например, `rating > 0` или `rating >= k`).

Если признака качества нет, можно использовать эвристики:

- исключить слишком короткие тексты (например, менее 1–2 предложений);
- исключить слишком длинные тексты (если вы хотите именно “короткие” анекдоты);
- удалить дубликаты.

> **Важно:** цель фильтрации — не идеально очистить все данные, а заметно повысить долю качественных примеров, чтобы обучение было осмысленным.

### Очистка текста анекдота

В пользовательских корпусах часто встречаются артефакты:

- невалидные символы (`&!`, `\r\n`, HTML-сущности),
- множественные пробелы,
- “склейки” нескольких анекдотов в один.

Рекомендуется привести тексты к более “чистому” виду:

- заменить `\r\n` на `\n` или пробелы,
- убрать лишние пробелы и неотображаемые символы,
- (опционально) удалить HTML-разметку, если она присутствует.

При этом важно не “перестараться”: в анекдотах часто используются тире, кавычки и переносы строк, и их не всегда нужно полностью удалять.

### Формирование промпта

В задаче генерации анекдотов удобно задать промпт в стиле:

- “Расскажи мне анекдот про студентов!”
- “Расскажи короткий анекдот про математику!”
- “Расскажи анекдот про футбол или спорт!”

Если в исходной таблице есть категориальный признак (`theme`), рекомендуется сопоставить каждому значению темы текстовый промпт.

### Форматирование в chat-формат (training_text)

Поскольку мы дообучаем chat-модель, каждой записи датасета следует сопоставить диалог:

- `system`: задаёт стиль и правила ответа (короткие смешные анекдоты),
- `user`: содержит промпт (тема),
- `assistant`: содержит целевой ответ (анекдот из датасета).

Рекомендуется хранить в датафрейме как минимум две строки:

- **training_text** — полный диалог (включая ответ ассистента): он нужен как “пример” правильного поведения, формируется с `add_generation_prompt=False`;
- **prompt_text** — только `system + user` с приглашением к генерации: он понадобится, чтобы при обучении считать лосс **только на ответе ассистента**, формируется с `add_generation_prompt=True`.

> Это различие важно: если не ограничивать лосс только ответом ассистента, модель может начать “учиться” продолжать также system/user часть диалога, что ухудшает поведение.

### Несколько слов про Dataset в экосистеме Hugging Face

В рамках курса вы будете использовать объект `datasets.Dataset`. Важно понимать следующее:

- он концептуально похож на таблицу (набор колонок одинаковой длины);
- он поддерживает быстрые преобразования “над всем датасетом” (например, `.map()`), которые удобны для токенизации и форматирования;
- он позволяет легко делать разбиение train/val (`train_test_split`) и выбирать поднабор (`select`) для быстрых тестов.

Рекомендуется **всегда** начинать с небольшого поднабора (smoke-test), чтобы проверить:

- корректность форматирования `training_text/prompt_text`,
- корректность токенизации,
- работоспособность обучения на 100–300 шагах,

и только потом запускать обучение на полном датасете.

## LoRA дообучение

В данной работе рекомендуется использовать **параметро-эффективное дообучение** (Parameter-Efficient Fine-Tuning, PEFT), а именно метод **LoRA** (Low-Rank Adaptation).

Главная идея LoRA состоит в том, что мы **не изменяем все веса большой модели**, а обучаем небольшие “адаптеры” (добавочные матрицы низкого ранга), которые модифицируют поведение модели. Таким образом получается, что:

- обучение становится существенно дешевле по времени и памяти;
- меньше риск сломать базовые языковые способности модели;
- результат обучения удобно сохранять в виде компактного набора параметров (LoRA-адаптера).

### Трейнеры в экосистеме Transformers / TRL / PEFT

При обучении LLM в экосистеме Hugging Face вы встретите инструментов, находящихся на разных уровнях абстракции:

- **transformers** — базовая библиотека, содержащая модели, токенизаторы и общий тренер `Trainer`;
- **peft** — библиотека с реализациями LoRA и других PEFT-методов;
- **trl** — библиотека для обучения LLM (включая supervised fine-tuning, а также обучение по предпочтениям).

В рамках данного задания рекомендуется использовать **TRL SFTTrainer + PEFT (LoRA)**, так как этот вариант:

- хорошо подходит для дообучения chat-моделей на корпусе вида "prompts -> ответы";
- позволяет подключать LoRA через конфиг (`peft_config`);
- даёт более компактный и понятный пайплайн обучения.

``` python

from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,                      
    args=training_args,             # ниже будет код, где показана инициализация параметров
    train_dataset=tokenized_train,          
    eval_dataset=tokenized_eval,
    processing_class=tokenizer,
    peft_config=peft_config,        # ниже будет код, где показана инициализация peft конфига   
)

```

> **Примечание:** в разных версиях библиотек `trl` и `transformers` могут отличаться имена аргументов и конфигураций. Ориентируйтесь на официальную документацию вашей версии.

### Основные этапы LoRA-SFT обучения

В корректно построенном пайплайне LoRA-SFT обычно есть четыре ключевых шага:

1. **Загрузка базовой модели** (decoder-only chat-модель) и токенизатора.
2. **Подготовка датасета** в виде текстов `training_text` + `prompt_text` и токенизация.
3. **Определение LoRA-конфигурации** (какие слои адаптируем и с какой “ёмкостью”).
4. **Запуск обучения** и последующая проверка на baseline-промптах.

### Важный нюанс: лосс только на ответ ассистента

Для chat-SFT важно, чтобы модель училась предсказывать именно **ответ ассистента**, а не повторять/продолжать подсказку пользователя и системное сообщение. Поэтому при формировании `labels` в батче рекомендуется:

- токены `system` и `user` части диалога исключать из лосса (проставлять `-100`);
- токены `assistant` части (сам анекдот) оставлять как целевую последовательность.

Такой подход, как правило, приводит к более стабильному и “правдоподобному” поведению чат-модели после дообучения.

Эти задачи можно реализовать с кастомном `DataCollator`, который выполняют ту же функцию, что и `collate_fn` в обычном PyTorch.

``` python

from dataclasses import dataclass
import torch

@dataclass
class ChatSFTCollator:
    tokenizer: any

    def __call__(self, features):
        prompt_lens = torch.tensor([f["prompt_len"] for f in features], dtype=torch.long)

        feats = [{k: v for k, v in f.items() if k != "prompt_len"} for f in features]

        batch = self.tokenizer.pad(feats, padding=True, return_tensors="pt")

        labels = batch["input_ids"].clone()
        labels[batch["attention_mask"] == 0] = -100  # padding

        for i, pl in enumerate(prompt_lens.tolist()):
            labels[i, :pl] = -100

        batch["labels"] = labels
        return batch

data_collator = ChatSFTCollator(tokenizer)
```

### Рекомендации по выбору гиперпараметров LoRA

LoRA имеет несколько ключевых гиперпараметров, которые отвечают за “силу” и устойчивость адаптации:

- **rank (`r`)** — “ёмкость” адаптера. Чем больше `r`, тем больше свободы у модели изменять поведение, но тем выше стоимость.
- **lora_alpha** — масштаб вклада LoRA (часто важна пропорция `alpha / r`).
- **lora_dropout** — регуляризация LoRA-ветки (полезна при шумных данных).

Рекомендуемые стартовые значения для задачи:

- `r = 8`
- `lora_alpha = 16`
- `lora_dropout = 0.05`

```python

from peft import LoraConfig

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
)

```

Если вы видите, что модель почти не меняет свое поведение после дообучения, можно увеличить `r` до 16 (при сохранении разумного learning rate). Если модель начинает генерировать странный текст или её стиль плавает, чаще помогает снизить общий learning rate и/или увеличить dropout для LoRA.

### Рекомендации по гиперпараметрам обучения (TrainingArguments / SFTConfig)

Ниже приведены типовые параметры, которые чаще всего требуется настраивать в учебной задаче:

#### Learning rate

Для LoRA-дообучения chat-модели в большинстве случаев рекомендуется начинать с умеренных значений learning rate, например порядка `1e-4`. Слишком большой learning rate может приводить к ухудшению языковой связности и появлению странного текста.

Также рекомендуется использовать небольшой warmup (например, несколько процентов от общего числа шагов), чтобы обучение начиналось более стабильно.

#### Размер батча и gradient accumulation

При обучении на GPU важно различать:

- **micro-batch** — размер батча, который реально помещается в память видеокарты;
- **effective batch** — итоговый “эффективный” батч, если используется накопление градиента (`gradient_accumulation_steps`).

Для ускорения обучения обычно полезно увеличивать micro-batch настолько, насколько позволяет память GPU, и только затем подбирать накопление градиента.

#### Частота eval / сохранений / логирования

В длительных экспериментах оценка на валидации и сохранение чекпоинтов могут заметно увеличивать время обучения. В учебной задаче рекомендуется:

- начинать со smoke-test без частых сохранений;
- использовать разумный интервал логирования;
- выполнять eval не слишком часто (например, раз в несколько сотен шагов или раз в эпоху).


Все эти параметры обычно задаются как инстанс специаьного класса:

```python

from transformers import TrainingArguments, Trainer

output_dir = "./tinyllama-anekdots-lora"

training_args = TrainingArguments(
    output_dir=output_dir,

    # --- батчи и обучение ---
    per_device_train_batch_size=2,  
    gradient_accumulation_steps=4,
    num_train_epochs=2,              

    learning_rate=1e-4,             
    warmup_ratio=0.003,               

    # --- логирование ---
    logging_steps=10,                   # как часто писать логи
    logging_strategy="steps",

    # --- валидация ---
    eval_strategy="epoch",              # "no" | "steps" | "epoch"
    save_strategy='no',
    eval_steps=50,                      # как часто считать eval_loss

    # --- типы и оптимизатор ---
    bf16=True,
    optim="paged_adamw_8bit",
    fp16=False,

    # --- логгеры ---
    report_to=["none"],                 # "none" | "tensorboard" | "wandb" | ["..."]
    run_name="tinyllama-anekdots",      # имя эксперимента в логах
)

def compute_metrics(eval_pred):
    """
    HF сюда передаёт (logits, labels), но для LM
    проще использовать уже готовый eval_loss из логов.
    Поэтому эту функцию можно оставить пустой или
    вернуть что-то простое.
    """
    return {}


```

### Рекомендуемая последовательность действий

Чтобы не тратить вычисления впустую, рекомендуется придерживаться следующего протокола:

1. **Smoke-test**:

    - взять небольшой поднабор данных;
    - выполнить короткое обучение (например, с иограниченны числом шагов);
    - убедиться, что loss уменьшается, а генерация меняется ожидаемым образом.

2. **Основной прогон**:

    - обучить модель на полном датасете (или на отфильтрованном “качественном” поднаборе);
    - сохранить LoRA-адаптер;
    - сравнить генерации на baseline-наборе промптов.

3. **Анализ и выводы**:

    - показать примеры “до/после”;
    - указать, какие параметры вы меняли и почему;
    - кратко обсудить влияние качества данных на результат.

### Сохранение результата обучения

После завершения обучения вам необходимо сохранить результат так, чтобы его можно было воспроизвести на другой машине или после перезапуска ноутбука. В случае LoRA это обычно означает сохранение:

- LoRA-адаптера,
- токенизатора,
- конфигурации обучения (параметров эксперимента).

> **Важно:** сохранение LoRA-адаптера отличается от сохранения полной модели: адаптер хранит только “разницу” относительно базовой модели. Для инференса требуется загрузить базовую модель и подключить к ней адаптер.
