##Описание задания

Вам предстоят реализовать **с нуля** (без автограда и готовых реализацией слоев) многослойный персептрон и обучить его на **Fashion-MNIST** для многоклассовой классификации (10 классов). Цель — сравнить, как **функции активации** (sigmoid, tanh, ReLU) и **схемы инициализации** (константой, случайными числами, Xavier, Kaiming) влияют на качество и стабильность обучения.

В результате у вас будет базовый код обучения сети, включающий прямой и обратный проход, обновление весов, логи метрик по эпохам, с помощью которого можно провести серию экспериментов

**Все решение должно быть в одном ноутбуке `solution.ipynb`.** Оно должен выполняться целиком **без ошибок** и быть воспроизводимым (нужно фиксировать `random_state`).

## Постановка задачи

**Пишите** весь код решения в блокноте `solution.ipynb`!

Разрешено использовать `numpy`, `matplotlib`, `sklearn.metrics` (для расчёта метрик). **Запрещено** использовать `torch.nn`, `torch.autograd`, `keras`, `jax`, любые оптимизаторы/слои/функции потерь из фреймворков.

### Создать класс `NeuralNetwork`

Создайте класс  `NeuralNetwork `(или `MLP`), экземпляр которого — полносвязная сеть с произвольным количеством слоев произвольной размерности. Его структура может выглядеть следующим образом:

```python
 
class MLP:

   def __init__(

        self,
        layers: list[int],              # напр.: [784, 100, 50, 10], 
        activation: str="relu",       # "sigmoid" | "tanh" | "relu"
        weight_init: str="xavier",    # "constant" | "normal" | "xavier" | "kaiming"
        dtype=np.float32,
        seed: int|None=42,

     ):
        ...


    def forward(self, X: np.ndarray) -> np.ndarray:
      """Возвращает логиты"""


    def backward(self, dL_dlogits: np.ndarray) -> None:
      """Считает dW, db"""


    def zero_grad(self) -> None:
      """Обнуляет dW, db (или просто очищает списки)."""


    def parameters(self) -> list[tuple[np.ndarray, np.ndarray]]:
      """[(W_0, b_0), ..., (W_{L-1}, b_{L-1})]. Удобно для шага СГД."""


    def predict(self, X: np.ndarray) -> np.ndarray:
        """Возвращает метки (argmax softmax(logits), axis=1)."""
```

Как интепретировать аргумент layers: это список интов, длина которого равна количество слоев, а i-ый элемент списка кодирует входную размерность i-го слоя.

Список сопутствущих задач:

1. Реализовать прямой и обратной проход
2. Реализовать четыре алгоритма инициализации сети: инициализация константным значением, значениями из нормального распределения, Xavier, Kaiming
3. Реализовать возможность применения в сети любой из трех функций активаций: сигмоиды, гиперболического тангсена и ReLU

### Обучение сети

1. Использовать для обучения датасет Fashion MNIST,  класс-обертку  `FashionMNIST`, находится в блокноте `solution.ipynb`
2. Реализовать softmax и функцию потерь кросс-энтропию
3. Реализовать стохастический градиентный спуск по мини-батчам (SGD)
4. После каждой train эпохи запускать val эпоху, без изменения весов модели
5. Во время обучения сети сохранять значения функции потерь, метрик классификации (accuracy, precision, recall, f1), L2 норму вектора градиента
6. После обучения модели визуализировать полученные кривые для train и val эпох
7. Зафиксировать seed для воспроизводимости экспериментов

### Серии экспериментов

Осуществите две серии экспериментов. Для первой используйте описания из таблицы ниже:

| Номер эксперимента | Конфигурация сети | Инициализация | Функция активации | Размер батча | Скорость обучения | Количество эпох |
| ----------------------------------- | --------------------------------- | -------------------------- | --------------------------------- | ----------------------- | --------------------------------- | ----------------------------- |
| 1                                   | [728, 100, 50, 10]                | random                     | sigmoid                           | 16                      | 0.1                               | 35                            |
| 2                                   | [728, 100, 50, 10]                | random                     | sigmoid                           | 32                      | 0.01                              | 15                            |
| 3                                   | [728, 392, 196, 98, 10]           | xavier                     | tanh                              | 32                      | 0.01                              | 15                            |
| 4                                   | [728, 392, 196, 98, 49, 25, 10]   | laiming                    | ReLU                              | 64                      | 0.01                              | 15                            |

Вторую серию экспериментов вы должны составить самостоятельно, чтобы проверить или опровергнуть следующие утверждения:

* выбор функции активации sigmoid в глубоких нейронных сетях негативно сказывается на обучении
* инициализация весов сети нулем или константой положительно сказывается на обучении
* инициализация весов сети Kaiming хороша для любой выбранной функции активации

## Критерии оценивания

**Работа будет оценена в 0 баллов в следующих случаях:**

- **Если решение НЕ в `solution.ipynb`**
- **Если блокнот находится в ненадлежащем виде**
- **Если результаты экспериментов НЕ сведены в электронную таблицу**
- **Если использован автоград/готовые слои/оптимизаторы (torch/keras/jax и т.п.)**

**Задача 1. Реализация MLP «с нуля» и корректность обучения: 4 балла**

**4** : есть класс с реализацией нейронной сети с произвольной архитектурой; реализованы `forward/backward` с корректным расчетом производных для всех функций активаций (сигмоида, гип. тангсенс, ReLU); реализовано обучение сети на кросс энтропию; реализованы все алгоритмы инициализации (constant, random, xavier, kaiming); обучение сети уменьшает функцию потерь и денмонстрирует, что классификатор работает лучше рандома (val accuracy > 0.1).

**2–3** : представлена реализация с недочетами (например, нет одной из инициализаций/активаций, ошибки в расчете градиентов, ошибка на части некоторых конфигураций).

**0–1** : существенные проблемы в реализации (нет обратного прохода, использован автоград, модель не обучается).

**Задача 2. Логирование метрик, визуализации и воспроизводимость: 3 балла.**

**3**: в конце каждой эпохи (обучающей и валидационной) сохраняются следующие значения: значение функции потерь, метрики классификации (accuracy, precision, recall, f1),  норма вектора градиента; в конце обучения выводятся читаемые графики.

**1–2** : логирование недополное (например, нет градиентной нормы или валид. метрик), графики есть, но неинформативны/без подписей; есть мелкие проблемы с воспроизводимостью.

**0** : логирование отсутствует/графики отсутствуют.

**Задача 3. Эксперименты и анализ гипотез: 3 балла.**

***3** : были выполнены обучения сети с 4-мя обязательные конфигурациями; были выполнения обучения сети для проверки трех утверждений, приведены текстовые комментарии; есть краткий анализ трёх утверждений (выводы со ссылками на графики/таблицы).

**1–2** : не все эксперименты из 4-х обязательных были выполнены; не все утверждения были экспериментально проверены, свобдная таблица не полна.

**0** : эксперименты не проведены.

**Шкала итоговой оценки**

- **Отлично (8–10):** решено задач на 8+ баллов.
- **Хорошо (6–7):** решено задач на 6–7 баллов.
- **Удовлетворительно (4–5):** решено задач на 4–5 баллов.
- **Неудовлетворительно (0–3):** решено задач менее чем на 4 балла.
