Следующий домен, который мы рассмотрим – аудиосигнал и задачи его обработки. Мы уже познакомились с компьютерным зрением, где модели учатся выделять скрытые признаки из изображений для решения ряда практических задач. Теперь настало время научить нейронные сети «слышать» и “понимать” звуковую информацию.

## Что такое аудиосигнал?

### Физика звука

Понятие звука обычно ассоциируется у нас со слухом и, следовательно, с физиологическими процессами в ушах, а также с психологическими процессами в нашем мозгу, где происходит переработка ощущений, поступающих в органы слуха). Кроме того, под *звуком* мы понимаем физическое явление, вызывающее действие на наши уши, а именно продольные волны.

**(def) Звук** – физическое явление, представляющее собой распространение продольных волн в газообразной, жидкой или твердой среде.

Любой звук имеет **источник**, являющийся телом, совершающее механические колебания.

Звук **регистрируется** (воспринимается) ухом или специальным прибором.

### Физические характеристики звука

**Звуковая волна** с физической точки зрения представляет собой колебание давления или смещения частиц среды, которое можно описать волновой функцией. Простейший случай — **гармоническое колебание** (чистый тон), которое математически описывается синусоидой:

$$
s(t) = A * sin(2πft + \phi)
$$

где:

- $s(t)$ — мгновенное значение звукового давления в момент времени t
- $A$ — **амплитуда** колебаний, определяющая громкость,
- $f$ — **частота** колебаний, определяющая высоту тона (в Герцах, Гц),
- $\phi$ — начальная фаза колебаний.

Эта формула является фундаментальной моделью, на которой основывается представление и анализ любого, даже самого сложного звукового сигнала.

Для слушающего человека сразу становятся очевидными две характеристики звука – **громкость** и **высота**. Они характеризуют *ощущения*, которые возникают в сознании слушателя. Однако каждой из этих субъективных характеристик соответствует величина, измеряемая физическими методами. Так громкость связана с интенсивностью звуковой волны, а высота – с частотой её колебаний.

#### Амплитуда и Интенсивность

**(def) Амплитуда** колебаний — это максимальное отклонение физической величины (например, давления) от среднего значения. Чем больше амплитуда, тем больше энергия, переносимая волной, и тем **громче** мы воспринимаем звук.

**(def) Интенсивность** определяется как средняя по времени энергия, переносимая волной за единицу времени через единичную площадь, перпендикулярную направлению волны. Она обозначается как $I$ и имеет единицу измерения (Вт/мт^2).

Для пло­ской си­ну­сои­даль­ной бе­гу­щей вол­ны интенсивность звука рав­на

$$
I=\frac{p^2}{2\rho c},
$$

где $p$ – ам­пли­ту­да зву­ко­во­го дав­ле­ния и ко­ле­ба­тель­ной ско­ро­сти час­тиц в сре­де, $\rho$ – плот­ность сре­ды, $с$ – ско­рость зву­ка в ней.

**Уровень интенсивности звука (Уровень громкости)**

Человеческое ухо способно воспринимать невероятно широкий диапазон интенсивностей — от примерно $10^{-12}$ Вт/м² (порог слышимости) до 1 Вт/м² и более (болевой порог). Работать с такими большими числами неудобно. Кроме того, наше восприятие громкости является не линейным, а **логарифмическим**. Чтобы привести шкалу интенсивностей к удобному, компактному виду, отражающему субъективное восприятие, используют логарифмические единицы — **децибелы (дБ)**.

**Уровень интенсивности** звука вычисляется по формуле:

$$
L = 10 * \log_{10} \Big(\frac{I}{I_0} \Big)
$$

где:

- $L$ — уровень интенсивности в децибелах (дБ),
- $I$ — интенсивность измеряемого звука,
- $I_0$ — пороговая интенсивность слышимости ($10^{-12}$ Вт/м² ).

Таким образом, порог слышимости соответствует 0 дБ, обычный разговор — около 60 дБ, а рок-концерт может достигать 110-120 дБ. Эта шкала позволяет удобно описать весь слышимый диапазон: изменение уровня на 10 дБ соответствует приблизительно двукратному изменению субъективной громкости.

#### Частота

Если амплитуда и интенсивность определяют **громкость** звука, то **частота** определяет его **высоту тона**.

**(def) Частота** — это количество полных колебаний (циклов) звуковой волны в секунду. Единица измерения — Герц (Гц). Один Герц равен одному колебанию в секунду. Чем выше частота, тем более высоким мы воспринимаем звук. Например, бас в музыке может иметь частоту 80 Гц, а сопрано — 1000 Гц и выше.

Человеческое ухо способно воспринимать частоты примерно в диапазоне от **20 Гц до 20 000 Гц (20 кГц)**. Этот диапазон называется **слышимым** или **акустическим**.

- **Инфразвук** — колебания ниже 20 Гц. Мы их не слышим, но можем ощущать физически.
- **Ультразвук** — колебания выше 20 кГц. Не слышим для человека, но используется животными (например, летучими мышами) и в различных технологиях (медицинская диагностика, эхолокация).

Важно понимать, что большинство реальных звуков, таких как речь или музыка, не являются чистыми тонами (синусоидами одной частоты). Они представляют собой сложные колебания, которые можно представить **как сумму множества простых синусоидальных волн с разными частотами и амплитудами**. Совокупность этих частот образует **акустический спектр** сигнала, который является фундаментальным понятием для цифровой обработки звука.

Об акустическом спектре мы поговорим чуть позже.

### Аналого-цифровое преобразование

!!! note "Теорема Котельникова-Найквиста"   
    Аналоговый сигнал с ограниченным спектром (т. е. со спектром, ограниченным некоторой частотой $f_{max}$) полностью определяется последовательностью своих дискретных значений (отсчётов), взятых через интервалы времени $\Delta t \leq \frac{1}{2 f_{max}}$, то есть с частотой дискретизации $f_d \geq 2f_{max}$

Для обработки звука с помощью компьютера необходимо преобразовать непрерывный аналоговый сигнал (звуковую волну) в дискретные цифровые значения. Этот процесс, называемый **аналого-цифровым преобразованием** (АЦП), состоит из трех ключевых этапов: дискретизации, квантования и кодирования.

1. **Дискретизация** – это процесс измерения амплитуды сигнала через равные промежутки времени. Количество таких измерений за одну секунду называется **частотой дискретизации** (sample rate) и измеряется в Герцах (Гц). Максимальная частота, которую можно корректно представить при данной частоте дискретизации, называется **частотой Котельникова** и равна половине частоты дискретизации. Например, человеческая речь в телефонной сети фильтруется до 4000 Гц, поэтому для её оцифровки достаточно частоты дискретизации 8000 Гц. Для более качественного звука стандартом является 16 000 Гц или 44 100 Гц (то, что можно услышать на CD дисках).
2. **Квантование** – это процесс замены реальных значений амплитуды на ближайшие значения из дискретного набора (уровней квантования). Разрядность квантования определяет точность этого представления. Чем больше бит используется, тем точнее представление амплитуды:
    1. **8-бит**: 256 возможных значений (от -128 до 127). К слову, вы наверняка хоть раз слышали характерный [“восьмибитный” звук](https://www.youtube.com/watch?v=1ebsX1VfjRg&list=PLVO9-kJCzLJ9Ito4B5ImX9JFBslqhO11z) старых видеоигр.
    2. **16-бит**: 65536 возможных значений (от -32768 до 32767)
3. **Кодирование**. На этапе **кодирования** квантованные значения преобразуются в битовые последовательности и упаковываются в файловый формат.

### Формат хранения аудиоданных WAV

Хотя существует множество аудиоформатов (как сжатых, таких как MP3, AAC, FLAC, так и несжатых), формат **WAV** (Waveform Audio File Format) является одним из наиболее распространенных стандартов для хранения несжатого аудио. Он был разработан совместно Microsoft и IBM и представляет собой подмножество более общего формата RIFF (Resource Interchange File Format).

Структура WAV-файла представима следующим образом:

1. **RIFF-заголовок** (12 байт):
    - `ChunkID`: 4 байта, всегда `"RIFF"`
    - `ChunkSize`: 4 байта, размер всего файла минус 8 байт
    - `Format`: 4 байта, всегда `"WAVE"`
2. **Форматный чанк** `"fmt "` (24 байта для стандартного PCM)
    - `Subchunk1ID`: 4 байта, всегда `"fmt "`
    - `Subchunk1Size`: 4 байта, размер оставшейся части форматного чанка (16 для PCM)
    - `AudioFormat`: 2 байта, тип кодирования (1 = PCM, другие значения = сжатие)
    - `NumChannels`: 2 байта, количество каналов (1 = моно, 2 = стерео)
    - `SampleRate`: 4 байта, частота дискретизации (Гц)
    - `ByteRate`: 4 байта, = SampleRate * NumChannels * BitsPerSample/8
    - `BlockAlign`: 2 байта, = NumChannels * BitsPerSample/8
    - `BitsPerSample`: 2 байта, разрядность (8, 16, 24, 32 бита)
3. **Чанк данных** `"data"`
    - `Subchunk2ID`: 4 байта, всегда `"data"`
    - `Subchunk2Size`: 4 байта, количество байтов в данных
    - `Data`: сами аудиоданные - последовательные отсчеты

## Спектральная форма представления сигнала

### Интуитивное понимание

Математическая форма изложения таких фундаментальных понятий, как акустический спектр, спектральные характеристики сигнала, крайне непросты. Поэтому перед ознакомлением с ними мы, как уже принято в рамках этого курса, постараемся получить интуитивное понимание этих понятий. Здесь нужно внимательно проследить за цепочкой рассуждений.

Пусть на временном отрезке $[a, b]$ задан аудиосигнал f(t). Если мы дискретизируем его равномерно по времени, получив $N$ отсчетов, то можем интерпретировать этот набор значений как **N-мерный вектор**:

$$
\mathbf{f} = (f[0], f[1], ..., f[N -1]) \in \mathbb{R}^N,
$$

При стремлении $N$ к бесконечности, $N \rightarrow \infty$, наша дискретная аппроксимация становится точным представлением исходного непрерывного сигнала.

По аналогии с тем, как двумерный вектор является точкой на плоскости, а трехмерный вектор – точкой в в трехмерном пространстве, **N-мерный вектор является точкой в N-мерном пространстве функций.**

Если мы представляем сигнал как вектор, то мы можем использовать для него знакомые геометрические понятия:

- норма вектора: $||f||^2 = \sum^{N-1}_{n=0}|f[n]|^2$, при  $N \rightarrow \infty$ эта сумма превращается в интеграл, $||f(t)|| = \sqrt{\int^b_a |f(t)|^2 dt}$
- скалярное произведение двух векторов, $f$ и $g$, определяется как $\langle f, g \rangle = ||f||g|| \cos \theta = f_1 g_1 + f_2 g_2 + ... + f_N g_N = \sum^N_{k=1} f_k g_k$, в непрерывном случае это будет $\langle f, g \rangle = \int^b_a f(t) g(t) dt$

Ключевая идея: в N-мерном пространстве мы можем выбрать **ортонормированный базис** — набор векторов ${e_1, e_2, ..., e_{N-1}}$, где

- $||e_k|| = 1$ (нормированность)
- $⟨e_i,e_j⟩ = 0$ при $i ≠ j$ (ортогональность)

Любой вектор в этом пространстве можно представить как линейную комбинацию базисных векторов:

$$
\mathbf{f} = \sum^{N-1}_{k=0} c_k \mathbf{e}_k, c_k  = \langle \mathbf{f}, \mathbf{e}_k\rangle
$$

В пространстве функций эта сумма превращается в интеграл, и мы ищем **ортонормированную систему функций**.

Наиболее естественным базисом для анализа периодических процессов является **система тригонометрических функций**:

$$
{1, sin(ωt), cos(ωt), sin(2ωt), cos(2ωt), ...}
$$

где $ω = 2π/T$ — основная частота.

Любой сигнал можно представить как сумму простых гармонических составляющих:

$$
f(t) = a_0 + ∑[a_k cos(kωt) + b_ksin(kωt)]
$$

где коэффициенты $a_k$ и $b_k$ показывают "вклад" каждой частоты в исходный сигнал.

**Идея разложения сигнала на составляющие**: сложный звук можно представить как сумму простых синусоидальных волн разных частот и амплитуд. Набор этих частот и их амплитуд и образует **акустический спектр** сигнала. Это преобразование из временной области в частотную и есть суть **преобразования Фурье**.

Такой подход позволяет анализировать, какие частоты присутствуют в сигнале и с какой интенсивностью, что невозможно сделать, глядя лишь на временную форму сигнала $f(t)$.

### Акустический спектр

**(def) Акустический спектр (спектр звука)** — со­во­куп­ность гар­мо­ни­че­ских ко­леба­ний (т.е. таких, при которых физ. величина изменяется с течением времением по синусоидальному/косинусоидалному закону), на ко­то­рые мож­но раз­ло­жить конкретную зву­ко­вую вол­ну.

Это представление описывает, **какие частоты** и **с какими амплитудами** присутствуют в сложном аудио сигнале.

### Преобразование Фурье

Преобразование Фурье (символ $\mathcal{F}$) — операция, сопоставляющая одной функции вещественной переменной другую функцию вещественной переменной.

Эта новая функция описывает коэффициенты («амплитуды») при разложении исходной функции на элементарные составляющие — гармонические колебания с разными частотами.

Для непрерывного сигнала $x(t)$ преобразование Фурье определяется как:

$$
X(\omega) = \mathcal{F}\{x(t)\} = \int_{-\infty}^{\infty} x(t) \cdot e^{-i\omega t}  dt,
$$

где:

- $X(\omega)$ — комплекснозначная функция, представляющая спектр
- $∣X(\omega)∣$ — амплитуда колебания на частоте $\omega$

Эта операция фактически вычисляет "коэффициенты" (амплитуды и фазы) при разложении исходной функции на элементарные составляющие — гармонические колебания с разными частотами.

Для дискретного сигнала x_n (полученного путём дискретизации) используется **Дискретное Преобразование Фурье (DFT)**:

$$
X_k = ∑_{n=0}^{N-1} x_n⋅ e^{(-i 2π k n / N)}
$$

для $k = 0, ..., N-1$.

### Быстрое преобразование Фурье

Прямое, поэлементное вычисление дискретного преобразования Фурье (ДПФ) по формуле имеет **вычислительную сложность $O(N^2)$**, что делает его непрактичным для больших значений $N$ (например, для аудиосигнала длительностью в несколько секунд, где $N$ может достигать сотен тысяч).

**Быстрое преобразование Фурье (Fast Fourier Transform, FFT)** —высокоэффективный **алгоритм** для вычисления ДПФ.

- **Сложность FFT**: $O(N \log_2 N)$
- **Выигрыш в скорости**: Для $N = 1024$ ($2^{10}$) сложность падает с $\sim$ 1 миллиона операций до $\sim$ 10 тысяч. Для современных аудиоприложений, где $N$ может быть $2^{16} = 65536$ и более, это разница между вычислениями, занимающими секунды, и вычислениями, занимающими доли миллисекунды.

Именно благодаря FFT стало возможным спектральный анализ аудио в реальном времени, что является основой для большинства современных технологий — от шумоподавления в наушниках до распознавания речи.

### Как оценить изменение спектра во времени?

Аудиосигналы, такие как речь или музыка, являются **нестационарными** — их спектральные характеристики изменяются во времени. Преобразование Фурье, примененное ко всему сигналу целиком, дает лишь усредненный спектр, скрывающий временную динамику. Для анализа изменяющихся спектральных свойств используется подход **скользящего оконного анализа**.

#### Windowing (Оконное преобразование)

Основная идея: сигнал разбивается на небольшие перекрывающиеся сегменты (окна), и к каждому такому сегменту независимо применяется преобразование Фурье.

Фрагмент сигнала, извлеченный из каждого окна, называется **фреймом**.

**Параметры оконного преобразования:**

1. **Ширина окна** (длительность в миллисекундах) — определяет временное разрешение
2. **Смещение** (шаг между последовательными окнами) — определяет плотность анализа
3. **Оконная функция** — определяет форму окна и влияет на качество спектральной оценки

#### Оконные функции

Прямоугольное окно (когда мы просто "вырезаем" отрезок сигнала) создает резкие разрывы на границах, что приводит к возникновению **спектральных искажений** (эффект Гиббса) в частотной области.

Для минимизации этих артефактов используются **оконные функции**, которые плавно уменьшают амплитуду сигнала к краям окна. Наиболее распространенные:

**Окно Хэмминга** — наиболее популярный выбор в обработке аудио:
$w(n) = 0.54 - 0.46 \cdot \cos\left(\frac{2\pi n}{N-1}\right)$ для $n = 0, \ldots, N-1$

**Окно Ханна** (Хэннинга):
$w(n) = 0.5 \cdot \left(1 - \cos\left(\frac{2\pi n}{N-1}\right)\right)$

**Сравнение характеристик:**

- **Прямоугольное окно**: лучшее временно́е разрешение, но худшее частотное
- **Окно Хэмминга**: хороший компромисс между разрешением и динамическим диапазоном
- **Окно Ханна**: лучшее подавление боковых лепестков

#### Short-Time Fourier Transform (STFT)

Комбинация оконного преобразования и БПФ дает **Short Time Fourier Transform**:

$X(\tau, \omega) = \text{STFT}{x(t)} = \int [x(t) \cdot w(t-\tau)] \cdot e^{-i\omega t} dt$

где

- $w(t-\tau)$ — оконная функция, сдвинутая во времени
- $\tau$ — временной сдвиг окна
- $\omega$ — частота

Для дискретного случая:

$$
X[m, k] = \sum_{n=0}^{N-1} x[n] \cdot w[n-mH] \cdot e^{-i2\pi kn/N},
$$

где $H$ — шаг сдвига окна (в отсчетах).

Результат STFT — это **спектрограмма**, двумерное представление сигнала (время-частота-интенсивность), которое визуализирует эволюцию спектра во времени.

### Другие спектральные характеристики сигнала

#### Шкала Мел

Человеческий слух обладает нелинейной частотной характеристикой — мы лучше различаем изменения на низких частотах, чем на высоких. Для учета этой особенности была разработана **мел-шкала** (mel-scale), которая аппроксимирует нелинейное восприятие частоты человеком.

Эмпирическая формула для преобразования частоты в герцах в мелы:

$$
m = 2595 \cdot \log_{10}\left(1 + \frac{f}{700}\right),
$$

где:

- $f$ — частота в Герцах
- $m$ — частота в мелах

На основе этой шкалы строится **банк мел-фильтров** — набор треугольных полосовых фильтров, равномерно распределенных по мел-шкале. Каждый фильтр выделяет энергию сигнала в определенной полосе частот, соответствующей восприятию человека.

#### MFCC (Mel-Frequency Cepstral Coefficients)

**MFCC** — наиболее популярные признаки в распознавании речи. Их физический смысл связан с моделью производства речи:

1. **Источник возбуждения** (голосовые связки) создает периодические колебания
2. **Голосовой тракт** (рот, язык, губы) действует как фильтр, формирующий спектр
3. Результирующий сигнал представляет собой **свертку** источника и фильтра

MFCC позволяют разделить эти компоненты и выделить параметры фильтра (форму голосового тракта), которые наиболее информативны для распознавания фонем.

**Этапы вычисления MFCC:**

1. **Предобработка**: предварительное усиление высоких частот
2. **STFT**: получение спектрограммы сигнала
3. **Банк мел-фильтров**: применение треугольных фильтров в мел-шкале
4. **Логарифмирование**: $\log(E)$ — переход к логарифмической шкале (имитирует логарифмическое восприятие громкости)
5. **Дискретное косинусное преобразование (DCT)**:$c_n = \sum_{k=1}^{M} (\log E_k) \cdot \cos\left[n(k-0.5)\frac{\pi}{M}\right]$
   - Отделяет медленно меняющуюся огибающую спектра (форма голосового тракта) от быстро меняющейся информации об источнике
   - Первые 12-13 коэффициентов обычно содержат наиболее важную информацию для обработки речевого сигнала

#### Другие важные спектральные признаки

**Spectral Centroid (Спектральный центроид)**

$$
\text{Центроид} = \frac{\sum_k (f_k \cdot A_k)}{\sum_k A_k}
$$

- "Центр масс" спектра, характеризует яркость звука
- Высокие значения соответствуют "яркому" тембру

**Spectral Roll-off (Спектральный откат)**
Частота, ниже которой сосредоточено 85% (или другая доля) спектральной энергии. Характеризует форму спектра.

**Spectral Contrast (Спектральный контраст)**
Разница энергий между пиками и впадинами в различных частотных полосах. Полезен для различения музыки и речи.

**Chromagram (Хромаграмма)**
Представление, где частотная ось свернута в 12 полутоновых классов октавы. Критически важен для анализа музыки.

**Zero-Crossing Rate (Частота пересечения нуля)**
Количество переходов сигнала через ноль за единицу времени. Простой, но эффективный признак для различения звонких/глухих звуков и обнаружения ударных в музыке.

Эти признаки, наряду с MFCC, образуют богатый набор характеристик, которые могут использоваться как по отдельности, так и в комбинации для различных задач анализа аудио.

## Задачи обработки аудиосигнала

#### Speech Processing (Обработка речи)

**Automatic Speech Recognition (ASR)** — автоматическое распознавание речи, которое заключается в преобразовании речевого сигнала в текст. Современные системы ASR используют глубокое обучение для построения монолитных моделей, способных распознавать спонтанную речь с высокой точностью.

**Text-to-Speech (TTS)** — синтез, генерация речи. Современные нейросетевые системы генерируют естественно звучащую речь, учитывая интонацию, ритм и эмоциональную окраску.

**Другие задачи обработки речи:**

- Идентификация диктора
- Определение эмоций
- Распознавание ключевых слов

#### Music Processing (Обработка музыки)

Обработка музыкальных сигналов включает широкий спектр задач

- **Классификация жанров** и настроения музыки
- **Beat tracking** — обнаружение ритма и темпа
- **Source separation** — разделение смешанного аудио на составляющие (вокал, барабаны, бас)
- **Automatic transcription** — автоматическая нотная запись
- **Music recommendation** — рекомендательные системы для музыки

#### 3. Other Audio Tasks (Другие задачи)

- **Acoustic scene classification** — классификация акустических сцен (улица, офис, транспорт)
- **Sound event detection** — обнаружение и классификация звуковых событий (звонок, лай, сирена)
- **Audio enhancement** — улучшение качества звука (шумоподавление, восстановление)

## Поиск похожего трека с помощью автоэнкодера

### Описание задачи

Пусть перед нами стоит задача — создать самое простое, первоначальное решение аналога функции "Моя волна по треку", то есть подбор плейлиста с похожими треками на понравившуюся нам композицию.

Ключевая сложность: как количественно определить и оценить "похожесть" музыкальных треков, которые могут различаться по темпу, инструментовке, тональности, но при этом восприниматься как схожие по настроению или стилю?

### Алгоритм первоначального, простого решения (MVP)

1. **Фрагментация**: От каждого аудиотрека берем 30-секундный сегмент (например, из начала, середины или конца трека)
2. **Извлечение признаков**: Для каждого сегмента вычисляем фиксированный набор спектральных и временн**ы**х признаков. Например, может быть такой набор:
   - MFCC (12-20 коэффициентов)
   - Spectral Centroid
   - Spectral Roll-off
   - Zero-Crossing Rate
   - Темп (BPM)
   - Громкость (RMS)
3. **Агрегация**: Усредняем признаки по всему сегменту, получая вектор фиксированной размерности для каждого трека
4. **Сравнение**: Вычисляем косинусное близости между векторами признаков для каждого трека

Выражение косинусной близости выглядит следующим образом:

$$
\text{similarity}(A, B) = \cos(\theta )  = \frac{A \cdot B}{||A|| \space ||B||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
$$

Значения лежат в диапазоне [-1, 1], где:

- 1 означает идентичные векторы
- 0 означает ортогональность (нет схожести)
- -1 означает противоположность

Часто используют выражение косинусной близости:

$$
\text{distance} = 1 - \cos(\theta) = 1 - \frac{A \cdot B}{\|A\| \|B\|} = 1 - \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
$$

Таким образом:

- distance = 0 — векторы идентичны (максимальная похожесть)
- distance = 1 — векторы ортогональны (нет схожести)
- distance = 2 — векторы противоположны (максимальное различие)

### Получение эмбеддингов с помощью автоэкнодера

**Что такое эмбеддинги?**

Эмбеддинг – это результат процесса преобразования каких-либо данных в набор чисел, векторы.

Этот термин получил распространение в рамках решения задачи получения векторного представления для текста, но его можно использовать и в контексте обработки аудио и изображений.

**Что такое автоэкнодер?**

Автоэнкодер (или автокодировщик) — специальная архитектура искусственных нейронных сетей, позволяющая применять обучение без учителя. Она состоит из двух компонентов:

- **Энкодер** $g$: преобразует входные данные $x$ в скрытое представление (код) $h = g(x)$
- **Декодер** $f$: восстанавливает данные из скрытого представления $\hat{x} = f(h) = f(g(x))$

Прямой проход (инференс) автоэнкодера тривиален: он сначала “сжимает” входные данные, а затем “расжимает” их обратно (восстанавливает).

Архитектура автоэнкодера обычно следует структуре "бутылочного горлышка" (bottleneck), где размерность скрытого представления $h$ значительно меньше размерности входных данных.

Внутреннее, промежуточное отображение входных данные можно использовать как эмбеддинг.

**Обучение автоэнкодера**

Автоэнкодер обучается минимизировать разницу между исходными данными и их реконструкцией. Функция потерь обычно представляет собой MSE (Mean Squared Error):

$$
L(x, \hat{x}) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{x}_i)^2
$$

или MAE (Mean Absolute Error):

$$
L(x, \hat{x}) = \frac{1}{n} \sum_{i=1}^{n} |x_i - \hat{x}_i|
$$

После обучения, **энкодер** может использоваться независимо для преобразования аудио в эмбеддинги — компактные векторные представления, которые capture существенные характеристики звука. Эти эмбеддинги затем используются для поиска похожих треков через сравнение в векторном пространстве.

**Преимущество подхода с автоэнкодером**: сеть автоматически учится выделять наиболее информативные признаки аудио, без необходимости ручного конструирования и подбора признаков, как в MVP-решении.
